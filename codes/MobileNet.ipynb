{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Generator for Train and Validation is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import cv2 as cv\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "from keras import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#base model import\n",
    "from tensorflow.keras.applications import MobileNetV2 as base_mod"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Related Initialization (Path Defination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Train_Images'\n",
    "val_dir = 'Val_Images'\n",
    "test_dir = 'Test_Images'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Image Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "num_classes = 2\n",
    "base_ephoc = 50 \n",
    "base_ephoc_executed = base_ephoc\n",
    "finetune_ephoc = 200\n",
    "\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "finetune_learning_rate = 0.0000001\n",
    "\n",
    "model_name = 'Mobilenet'\n",
    "limb_type = 'All_Limbs'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255\n",
    "                                                                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Validation Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_data_generator.flow_from_directory(data_dir, \n",
    "                                            seed = 123,\n",
    "                                            target_size=(img_height,img_width),\n",
    "                                            batch_size=batch_size,\n",
    "                                            class_mode='sparse',\n",
    "                                            classes=['Intact','Fractured']\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = train_data_generator.flow_from_directory(val_dir, \n",
    "                                            seed = 123,\n",
    "                                            target_size=(img_height,img_width),\n",
    "                                            batch_size=batch_size,\n",
    "                                            class_mode='sparse',\n",
    "                                            classes=['Intact','Fractured']\n",
    "                                        )\n",
    "class_indices = train_ds.class_indices\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks and Stopping Point defination for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model checkpoint path\n",
    "model_checkpoint_path = os.path.join('Models', model_name, model_name + '_checkpoint/')\n",
    "\n",
    "#check for the folder path, if exists skip else create the path\n",
    "if not (os.path.exists(model_checkpoint_path)):\n",
    "    os.makedirs(model_checkpoint_path)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_point = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=15,\n",
    "    verbose=1,\n",
    "    mode=\"max\",\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate scheduler, reschedule the learning rate if the same validation accuracy is repeated for 5 times, the lowest learning rate is also set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.2,\n",
    "                              patience=5, min_lr=finetune_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_logs = tf.keras.callbacks.TensorBoard(log_dir=\"./tb_logs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data logger to retain the ephocs, train, and validation metrices during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for the file for saving heatmap\n",
    "csv_save_path = 'Graphs/' + model_name\n",
    "csv_save_file = csv_save_path + '/' + model_name + '_' + limb_type +'_train_data.csv'\n",
    "csv_save_file2 = csv_save_path + '/' + model_name + '_' + limb_type +'_fine_data.csv'\n",
    "\n",
    "if not (os.path.exists(csv_save_path)):\n",
    "    os.makedirs(csv_save_path)\n",
    "\n",
    "data_logger = tf.keras.callbacks.CSVLogger(csv_save_file, \n",
    "                                            separator= ',',\n",
    "                                            append=True\n",
    "                                            )\n",
    "data_logger2 = tf.keras.callbacks.CSVLogger(csv_save_file2, \n",
    "                                            separator= ',',\n",
    "                                            append=True\n",
    "                                            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = base_mod(input_shape=(img_height,img_width,3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get last layer from base_model and set that as the input for the layers to be stacked on top of the base layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_base_layer = base_model.get_layer('out_relu')\n",
    "last_layer_output = last_base_layer.output\n",
    "added_layer = tf.keras.layers.GlobalAveragePooling2D()(last_layer_output)\n",
    "added_layer = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(added_layer)\n",
    "added_layer = tf.keras.layers.Dense(1024, activation='relu')(added_layer)\n",
    "added_layer = tf.keras.layers.Dropout(0.4, noise_shape=None, seed=None)(added_layer)\n",
    "added_layer = tf.keras.layers.Dense(1024, activation='relu')(added_layer)\n",
    "added_layer = tf.keras.layers.Dropout(0.4, noise_shape=None, seed=None)(added_layer)\n",
    "added_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(added_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assembling the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(base_model.input, added_layer)\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=base_learning_rate),\n",
    "                loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics = ['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the diagram of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, to_file='Graphs/efficient.jpg', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = base_ephoc  \n",
    "history_base = model.fit(train_ds, validation_data=val_ds, epochs=base_ephoc, callbacks=[model_checkpoint_callback, stopping_point, data_logger, reduce_lr, tensorboard_logs])\n",
    "base_ephoc_executed = len(history_base.history['loss'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(acc,val_acc,type):\n",
    "    #plot for the accuracy\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(acc,label='Training Accuracy')\n",
    "    plt.plot(val_acc,label='Validation Accuracy')\n",
    "    #plt.plot([base_ephoc_executed-1, base_ephoc_executed-1], plt.ylim(), label='Start Fine Tuning')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    #plt.show()\n",
    "\n",
    "    #define the model checkpoint path\n",
    "    figure_save_path = 'Graphs/' + model_name\n",
    "    figure_save_name =  figure_save_path + '/' + model_name + '_' + limb_type + type +'_accuracy.jpg'\n",
    "\n",
    "    #check for the folder path, if exists skip else create the path\n",
    "    if not(os.path.exists(figure_save_path)):\n",
    "        os.makedirs(figure_save_path)\n",
    "\n",
    "    plt.savefig(figure_save_name, dpi=200)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss,val_loss,type):\n",
    "    #plotting the data from ephoc to\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(loss,label='Training Loss')\n",
    "    plt.plot(val_loss,label='Validation Loss')\n",
    "    #plt.plot([base_ephoc_executed-1, base_ephoc_executed-1], plt.ylim(), label='Start Fine Tuning')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    #plt.show()\n",
    "\n",
    "    #define the model checkpoint path\n",
    "    figure_save_path = 'Graphs/' + model_name\n",
    "    figure_save_name =  figure_save_path + '/' + model_name + '_' + limb_type + type +'_loss.jpg'\n",
    "\n",
    "    #check for the folder path, if exists skip else create the path\n",
    "    if not(os.path.exists(figure_save_path)):\n",
    "        os.makedirs(figure_save_path)\n",
    "\n",
    "    plt.savefig(figure_save_name, dpi=200)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the base acc and loss with the fintuned loss and acc to get the whole of the acc and loss\n",
    "acc = history_base.history['accuracy']\n",
    "val_acc = history_base.history['val_accuracy']\n",
    "\n",
    "loss = history_base.history['loss']\n",
    "val_loss = history_base.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(acc,val_acc,'base')\n",
    "plot_loss(loss,val_loss,'base')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning the Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfreeze all the layers except batchnorm layers from block6c add layers onwards for fine tuning\n",
    "for layer in model.layers[-81:-1]:\n",
    "    if not isinstance(layer, layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=finetune_learning_rate),\n",
    "                loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ephocs = base_ephoc_executed + finetune_ephoc\n",
    "\n",
    "history_fine_tune = model.fit(train_ds, validation_data=val_ds, \n",
    "                                epochs=total_ephocs, \n",
    "                                initial_epoch = history_base.epoch[-1],\n",
    "                                callbacks=[model_checkpoint_callback,stopping_point,data_logger2, reduce_lr, tensorboard_logs])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the model performance after the fine tuning ephocs (Without the MARKER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine_tune.history['accuracy']\n",
    "val_acc += history_fine_tune.history['val_accuracy']\n",
    "\n",
    "loss += history_fine_tune.history['loss']\n",
    "val_loss += history_fine_tune.history['val_loss']\n",
    "\n",
    "\n",
    "#plotting the data from ephoc to\n",
    "plot_accuracy(acc,val_acc,'finetune')\n",
    "plot_loss(loss,val_loss,'finetune')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the final trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model checkpoint path\n",
    "model_save_path = os.path.join('Models/' + model_name)\n",
    "\n",
    "#check for the folder path, if exists skip else create the path\n",
    "if not (os.path.exists(model_save_path)):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "#create a path for model name\n",
    "model_save_file = model_save_path + '/' + model_name + '_' + limb_type +'.h5'\n",
    "\n",
    "if os.path.exists(model_save_file):\n",
    "    pass\n",
    "\n",
    "model.save(model_save_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Pridiction Using the Model in Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(model_save_file)\n",
    "class_subset = sorted(os.listdir('Test_Images'), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_data_generator.flow_from_directory(test_dir,\n",
    "                                             target_size=(img_height, img_width),\n",
    "                                             batch_size=1,\n",
    "                                             class_mode='sparse',\n",
    "                                             shuffle=False,\n",
    "                                             classes=['Intact','Fractured'],\n",
    "                                             seed=123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Classes of the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = test_ds.classes\n",
    "class_indices = train_ds.class_indices\n",
    "class_indices = dict((v,k) for k,v in class_indices.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_ds)\n",
    "pred_classes = np.argmax(prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, auc, classification_report, confusion_matrix, balanced_accuracy_score, roc_curve\n",
    "import seaborn as sns\n",
    "\n",
    "#calculating each of the metrices\n",
    "accuracy = accuracy_score(true_classes, pred_classes)\n",
    "balanced_accuracy = balanced_accuracy_score(true_classes, pred_classes)\n",
    "precision = precision_score(true_classes, pred_classes)\n",
    "recall = recall_score(true_classes, pred_classes)\n",
    "f1 = f1_score(true_classes, pred_classes)\n",
    "auc_mod = roc_curve(true_classes, pred_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix, Accuracy Scores, Classification Reports and other stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(true_classes, pred_classes)\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax = sns.heatmap(conf_mat, annot=True, cbar=False, square=True, fmt='d', cmap=plt.cm.Blues, xticklabels=class_subset, yticklabels=class_subset,annot_kws={'size': 18})\n",
    "heatmap = ax.get_figure()\n",
    "\n",
    "#check for the file for saving heatmap\n",
    "figure_save_path = 'Graphs/' + model_name + '/'\n",
    "figure_save_name = figure_save_path + model_name + '_' + limb_type +'_fined_tuned_confusion_matrix.jpg'\n",
    "\n",
    "#check for the folder path, if exists skip else create the path\n",
    "if not (os.path.exists(figure_save_path)):\n",
    "    os.makedirs(figure_save_path)\n",
    "\n",
    "heatmap.savefig(figure_save_name, dpi=200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Accuracy For Given Model on Test Datset: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Balanced Accuracy For Given Model on Test Datset: {:.2f}%\".format(balanced_accuracy * 100))\n",
    "print(\"Precision For Given Model on Test Datset: {:.2f}%\".format(precision * 100))\n",
    "print(\"Recall For Given Model on Test Datset: {:.2f}%\".format(recall * 100))\n",
    "print(\"F1-Score For Given Model on Test Datset: {:.2f}%\".format(f1 * 100))\n",
    "\n",
    "\n",
    "#dictionary of the metrices\n",
    "metrices = {'Accuracy' : [accuracy], 'Balanced_Accuracy': [balanced_accuracy], 'Precision': [precision], 'Recall': [recall], 'F1_Score': [f1], 'AUC': [auc_mod]}\n",
    "\n",
    "#create a dataframe of the metrices\n",
    "df = pd.DataFrame.from_dict(metrices, orient='columns', dtype=None, columns=None)\n",
    "\n",
    "#check for the file for saving metrices\n",
    "file_save_path = 'Graphs/' + model_name + '/'\n",
    "file_save_name = file_save_path + model_name + '_' + limb_type +'_metrices.csv'\n",
    "\n",
    "#check for the folder path, if exists skip else create the path\n",
    "if not (os.path.exists(file_save_path)):\n",
    "    os.makedirs(file_save_path)\n",
    "\n",
    "df.to_csv(file_save_name, index=None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "classi_report = classification_report(true_classes, pred_classes, output_dict=True)\n",
    "\n",
    "classfication_report_df = pd.DataFrame(classi_report).transpose()\n",
    "\n",
    "\n",
    "#check for the file for saving heatmap\n",
    "file_save_path = 'Graphs/' + model_name + '/'\n",
    "file_save_name = file_save_path + model_name + '_' + limb_type +'_Classification_Report.csv'\n",
    "\n",
    "#check for the folder path, if exists skip else create the path\n",
    "if not (os.path.exists(file_save_path)):\n",
    "    os.makedirs(file_save_path)\n",
    "\n",
    "classfication_report_df.to_csv(file_save_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
